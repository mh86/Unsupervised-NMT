{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the raw data\n",
    "In this notebook, we will create a dataset after having processed the raw data using Facebook's tools they have pooled together (the news crawl data, mosescoder tokenizer, fastBPE compression).  \n",
    "\n",
    "The official API is [here](https://github.com/facebookresearch/XLM).  I used Microsoft's API instead found [here](https://github.com/xutaatmicrosoftdotcom/MASS-1#data-ready).\n",
    "\n",
    "The relevant portions I have used are as follows:\n",
    "Download the github [repository](https://github.com/facebookresearch/XLM) and then the following (via terminal, after going into your project folder):\n",
    "```\n",
    "wget https://dl.fbaipublicfiles.com/XLM/codes_enfr\n",
    "wget https://dl.fbaipublicfiles.com/XLM/vocab_enfr\n",
    "```\n",
    "Then run the bash file that extracts the necessary data and processes the data (~10 GB space needed).  You might want to activate any virtual environments you might be using as this will utilize certain python packages.  I also had to go into the bash file and added\n",
    "```\n",
    "python \n",
    "```\n",
    "in front of command lines that involved running the \"preprocess.py\" file.  We will be working with the English language as the source and the French language as the target.\n",
    "```\n",
    "./get-data-nmt.sh --src en --tgt fr --reload_codes codes_enfr --reload_vocab vocab_enfr\n",
    "```\n",
    "You should get a readout at the end indicating the locations of your train, valid, and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x80\\x04\\x95\\r\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8a'\n",
      "b'l\\xfc\\x9cF\\xf9 j\\xa8P\\x19.\\x80\\x04\\x95\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00M\\xe9\\x03.\\x80\\x04\\x95X\\x00\\x00\\x00\\x00\\x00\\x00\\x00}\\x94(\\x8c\\x10protocol_version\\x94M\\xe9\\x03\\x8c\\rlittle_endian\\x94\\x88\\x8c'\n",
      "b'type_sizes\\x94}\\x94(\\x8c\\x05short\\x94K\\x02\\x8c\\x03int\\x94K\\x04\\x8c\\x04long\\x94K\\x04uu.\\x80\\x04\\x95\\x02\\x00\\x01\\x00\\x00\\x00\\x00\\x00}\\x94(\\x8c\\x04dico\\x94\\x8c\\x13src.data.dictionary\\x94\\x8c'\n",
      "b'Dictionary\\x94\\x93\\x94)\\x81\\x94}\\x94(\\x8c\\x07id2word\\x94}\\x94(K\\x00\\x8c\\x03<s>\\x94K\\x01\\x8c\\x04</s>\\x94K\\x02\\x8c\\x05<pad>\\x94K\\x03\\x8c\\x05<unk>\\x94K\\x04\\x8c'\n",
      "b'<special0>\\x94K\\x05\\x8c'\n",
      "b'<special1>\\x94K\\x06\\x8c'\n",
      "b'<special2>\\x94K\\x07\\x8c'\n",
      "b'<special3>\\x94K\\x08\\x8c'\n",
      "b'<special4>\\x94K\\t\\x8c'\n",
      "b'<special5>\\x94K'\n",
      "b'\\x8c'\n",
      "b'<special6>\\x94K\\x0b\\x8c'\n",
      "b'<special7>\\x94K\\x0c\\x8c'\n",
      "b'<special8>\\x94K\\r\\x8c'\n",
      "b'<special9>\\x94K\\x0e\\x8c\\x01,\\x94K\\x0f\\x8c\\x01.\\x94K\\x10\\x8c\\x03the\\x94K\\x11\\x8c\\x01a\\x94K\\x12\\x8c\\x02to\\x94K\\x13\\x8c\\x01\"\\x94K\\x14\\x8c\\x02of\\x94K\\x15\\x8c\\x03and\\x94K\\x16\\x8c\\x02in\\x94K\\x17\\x8c\\x02de\\x94K\\x18\\x8c\\x02\\'s\\x94K\\x19\\x8c\\x04that\\x94K\\x1a\\x8c\\x02on\\x94K\\x1b\\x8c\\x03for\\x94K\\x1c\\x8c\\x02la\\x94K\\x1d\\x8c\\x02is\\x94K\\x1e\\x8c\\x03The\\x94K\\x1f\\x8c\\x02\\xc3\\xa0\\x94K \\x8c\\x03was\\x94K!\\x8c\\x02le\\x94K\"\\x8c\\x04with\\x94K#\\x8c\\x02l\\'\\x94K$\\x8c\\x04said\\x94K%\\x8c\\x02et\\x94K&\\x8c\\x02it\\x94K\\'\\x8c\\x02as\\x94K(\\x8c\\x03les\\x94K)\\x8c\\x02at\\x94K*\\x8c\\x01:\\x94K+\\x8c\\x03des\\x94K,\\x8c\\x02en\\x94K-\\x8c\\x02d\\'\\x94K.\\x8c\\x02he\\x94K/\\x8c\\x02by\\x94K0\\x8c\\x01)\\x94K1\\x8c\\x02be\\x94K2\\x8c\\x01(\\x94K3\\x8c\\x01I\\x94K4\\x8c\\x04have\\x94K5\\x8c\\x04from\\x94K6\\x8c\\x01-\\x94K7\\x8c\\x03his\\x94K8\\x8c\\x03has\\x94K9\\x8c\\x03are\\x94K:\\x8c\\x02un\\x94K;\\x8c\\x02an\\x94K<\\x8c\\x02du\\x94K=\\x8c\\x03est\\x94K>\\x8c\\x03not\\x94K?\\x8c\\x03une\\x94K@\\x8c\\x04will\\x94KA\\x8c\\x03who\\x94KB\\x8c\\x04pour\\x94KC\\x8c\\x03but\\x94KD\\x8c\\x03had\\x94KE\\x8c\\x04they\\x94KF\\x8c\\x04this\\x94KG\\x8c\\x03que\\x94KH\\x8c\\x01\\'\\x94KI\\x8c\\x05their\\x94KJ\\x8c\\x03qui\\x94KK\\x8c\\x04dans\\x94KL\\x8c\\x04were\\x94KM\\x8c\\x04been\\x94KN\\x8c\\x02or\\x94KO\\x8c\\x04more\\x94KP\\x8c\\x02au\\x94KQ\\x8c\\x02\\'t\\x94KR\\x8c\\x05which\\x94KS\\x8c\\x05about\\x94KT\\x8c\\x03you\\x94KU\\x8c\\x02we\\x94KV\\x8c\\x03her\\x94KW\\x8c\\x05would\\x94KX\\x8c\\x02It\\x94KY\\x8c\\x02up\\x94KZ\\x8c\\x03one\\x94K[\\x8c\\x03its\\x94K\\\\\\x8c\\x03sur\\x94K]\\x8c\\x05after\\x94K^\\x8c\\x03out\\x94K_\\x8c\\x03par\\x94K`\\x8c\\x03pas\\x94Ka\\x8c\\x01?\\x94Kb\\x8c\\x02il\\x94Kc\\x8c\\x03can\\x94Kd\\x8c\\x03she\\x94Ke\\x8c\\x01$\\x94Kf\\x8c\\x04also\\x94Kg\\x8c\\x04than\\x94Kh\\x8c\\x04when\\x94Ki\\x8c\\x03son\\x94Kj\\x8c\\x06people\\x94Kk\\x8c\\x03all\\x94Kl\\x8c\\x01A\\x94Km\\x8c\\x02He\\x94Kn\\x8c\\x04plus\\x94Ko\\x8c\\x04year\\x94Kp\\x8c\\x02Le\\x94Kq\\x8c\\x04over\\x94Kr\\x8c\\x03But\\x94Ks\\x8c\\x05there\\x94Kt\\x8c\\x04into\\x94Ku\\x8c\\x04time\\x94Kv\\x8c\\x02se\\x94Kw\\x8c\\x03qu\\'\\x94Kx\\x8c\\x02In\\x94Ky\\x8c\\x03two\\x94Kz\\x8c\\x05first\\x94K{\\x8c\\x02ce\\x94K|\\x8c\\x04last\\x94K}\\x8c\\x02s\\'\\x94K~\\x8c\\x02ne\\x94K\\x7f\\x8c\\x03new\\x94K\\x80\\x8c\\x03ont\\x94K\\x81\\x8c\\x05could\\x94K\\x82\\x8c\\x04avec\\x94K\\x83\\x8c\\x02so\\x94K\\x84\\x8c\\x02n\\'\\x94K\\x85\\x8c\\x05years\\x94K\\x86\\x8c\\x04what\\x94K\\x87\\x8c\\x05other\\x94K\\x88\\x8c\\x04just\\x94K\\x89\\x8c\\x01/\\x94K\\x8a\\x8c\\x02We\\x94K\\x8b\\x8c\\x04them\\x94K\\x8c\\x8c\\x04some\\x94K\\x8d\\x8c\\x04like\\x94K\\x8e\\x8c\\x02if\\x94K\\x8f\\x8c\\x01%\\x94K\\x90\\x8c\\x02no\\x94K\\x91\\x8c\\x03him\\x94K\\x92\\x8c\\x05\\xc3\\xa9t\\xc3\\xa9\\x94K\\x93\\x8c\\x01;\\x94K\\x94\\x8c\\x02La\\x94K\\x95\\x8c\\x02do\\x94K\\x96\\x8c\\x03our\\x94K\\x97\\x8c\\x04only\\x94K\\x98\\x8c\\x01y\\x94K\\x99\\x8c\\x03aux\\x94K\\x9a\\x8c\\x03now\\x94K\\x9b\\x8c\\x03Les\\x94K\\x9c\\x8c\\x04back\\x94K\\x9d\\x8c\\x06before\\x94K\\x9e\\x8c\\x02my\\x94K\\x9f\\x8c\\x04sont\\x94K\\xa0\\x8c\\x07because\\x94K\\xa1\\x8c\\x04says\\x94K\\xa2\\x8c\\x02me\\x94K\\xa3\\x8c\\x04most\\x94K\\xa4\\x8c\\x03get\\x94K\\xa5\\x8c\\x05being\\x94K\\xa6\\x8c\\x04told\\x94K\\xa7\\x8c\\x05where\\x94K\\xa8\\x8c'\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./data/processed/en-fr\"\n",
    "with open(data_path+\"/train.en.pth\", mode='rb') as f:\n",
    "    count = 0\n",
    "#     print(f)\n",
    "    for line in f:\n",
    "        print(line.rstrip())\n",
    "        count += 1\n",
    "        if count == 15:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reads like nonsense.  However, this is just bytes encoding of our original text.  Let us look at the contents of the data after loading it in torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['dico', 'positions', 'sentences', 'unk_words'])\n",
      "[ 5543  1569 13902  7541    14 17242  2729    14    59 21255  4610  1936\n",
      "     1   202    52    62    77  1396    78  8528  1337    15     1    19\n",
      "  1010  1374    81   493    59 18566  3342    14    62    41   107    14\n",
      "    19  9433    36   404    15     1]\n",
      "[[  0  12]\n",
      " [ 13  22]\n",
      " [ 23  41]\n",
      " [ 42  61]\n",
      " [ 62  90]\n",
      " [ 91 127]\n",
      " [128 157]\n",
      " [158 177]\n",
      " [178 215]\n",
      " [216 241]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.load(data_path+\"/train.en.pth\")\n",
    "print(data.keys())\n",
    "print(data['sentences'][0:42])\n",
    "print(data['positions'][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This whole process broke down the corpus into indices of words and positions demarcating the sentences.  Now a dataset loader will need to take this in and break it down into batches for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusDataset():\n",
    "    \n",
    "    def __init__(self, int_sent, pos, batch_size=256):\n",
    "        \"\"\"\n",
    "        Constructs a dataset that can iterate over batches of sentences.\n",
    "        :param int_sent : nd_array containing integer versions of the entire corpus\n",
    "        :param pos : nd_array of pairs containing starting position and \n",
    "                     ending position of each sentence\n",
    "        :param batch_size : batch size with default value of 256\n",
    "        \"\"\"\n",
    "        self.int_sent = int_sent\n",
    "        self.pos = pos\n",
    "        self.batch_size = batch_size\n",
    "        self.num_sent = len(self.pos)\n",
    "        \n",
    "    def batch_iterator(self,):\n",
    "        ## TODO:\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_sent\n",
    "\n",
    "    def _process_batch(self,):\n",
    "        ## TODO:\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        ## TODO: \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
